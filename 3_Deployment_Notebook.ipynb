{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Deployment-Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MPcX1AQHSr6i",
        "aOp_OoKASvYq",
        "5cxtcKKgStPf",
        "TL109phzSvDJ"
      ],
      "authorship_tag": "ABX9TyMCc1kn4NXW5OIf01PRbpEs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayman947/HR-LogisticRegression-PredictingAbsenteeism/blob/main/3_Deployment_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPcX1AQHSr6i"
      },
      "source": [
        "### Importing All Necessary **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFb_408WSs6f"
      },
      "source": [
        "# importing the relevant libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "# create the Custom Scaler class to only scale predetermined columns\n",
        "\n",
        "class CustomScaler(BaseEstimator,TransformerMixin): \n",
        "    \n",
        "    # init or what information we need to declare a CustomScaler object\n",
        "    # and what is calculated/declared as we do\n",
        "    \n",
        "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
        "        \n",
        "        # scaler is nothing but a Standard Scaler object\n",
        "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
        "        # with some columns 'twist'\n",
        "        self.columns = columns\n",
        "        self.mean_ = None\n",
        "        self.var_ = None\n",
        "        \n",
        "    \n",
        "    # the fit method, which, again based on StandardScale\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        self.scaler.fit(X[self.columns], y)\n",
        "        self.mean_ = np.mean(X[self.columns])\n",
        "        self.var_ = np.var(X[self.columns])\n",
        "        return self\n",
        "    \n",
        "    # the transform method which does the actual scaling\n",
        "\n",
        "    def transform(self, X, y=None, copy=None):\n",
        "        \n",
        "        # record the initial order of the columns\n",
        "        init_col_order = X.columns\n",
        "        \n",
        "        # scale all features that you chose when creating the instance of the class\n",
        "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
        "        \n",
        "        # declare a variable containing all information that was not scaled\n",
        "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
        "        \n",
        "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
        "        # use the original order (that you recorded in the beginning)\n",
        "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOp_OoKASvYq"
      },
      "source": [
        "### **Unpickling** The Model & Scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbEKgutUrLi"
      },
      "source": [
        "with open('reg', 'rb') as model_file, open('scaler', 'rb') as scaler_file:\n",
        "  reg  = pickle.load(model_file)\n",
        "  scaler = pickle.load(scaler_file)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cxtcKKgStPf"
      },
      "source": [
        "### Importing The **New Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRKi76flStX3"
      },
      "source": [
        "raw_new = pd.read_csv('3_absenteeism_new_data.csv')\n",
        "Raw_new_data = raw_new.copy()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL109phzSvDJ"
      },
      "source": [
        "### Data **Cleansning & Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmXbDd0_SvPY",
        "outputId": "c1cd9d3d-feb7-4702-8303-b1cb1a1e7803"
      },
      "source": [
        "Raw_new_data = Raw_new_data.drop(['ID'], axis = 1)\n",
        "reason_columns = pd.get_dummies(Raw_new_data['Reason for Absence'], drop_first = True)\n",
        "Raw_new_data = Raw_new_data.drop(['Reason for Absence'], axis = 1)\n",
        "reason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)\n",
        "reason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)\n",
        "reason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)\n",
        "reason_type_4 = reason_columns.loc[:, 22:].max(axis=1)\n",
        "Raw_new_data = pd.concat([Raw_new_data, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis = 1)\n",
        "Raw_new_data.columns = ['Date', 'Transportation Expense', 'Distance to Work', 'Age','Daily Work Load Average', 'Body Mass Index', 'Education','Children', 'Pets', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4']\n",
        "column_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Date', 'Transportation Expense', 'Distance to Work', 'Age','Daily Work Load Average', 'Body Mass Index', 'Education','Children', 'Pets']\n",
        "Raw_new_data = Raw_new_data[column_names_reordered]\n",
        "Raw_new_data['Reason_2'] = 0\n",
        "df_reason_mod = Raw_new_data\n",
        "df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'])    \n",
        "df_reason_mod['Month'] = df_reason_mod['Date'].apply(lambda x: x.month)\n",
        "df_reason_mod['Day of the Week'] = df_reason_mod['Date'].apply(lambda x: x.weekday())\n",
        "column_names_upd = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month', 'Day of the Week','Transportation Expense', 'Distance to Work', 'Age','Daily Work Load Average', 'Body Mass Index', 'Education', 'Children','Pets']\n",
        "df_reason_date_mod = df_reason_mod[column_names_upd]\n",
        "df_reason_date_mod['Education'] = df_reason_date_mod['Education'].map({1:0, 2:1, 3:1, 4:1})\n",
        "data_preprocessed = df_reason_date_mod.copy()\n",
        "data_preprocessed.drop(['Daily Work Load Average', 'Distance to Work', 'Day of the Week'], axis = 1, inplace=True)\n",
        "unscaled_inputs = data_preprocessed.iloc[:,:]\n",
        "scaled_inputs = scaler.transform(unscaled_inputs)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ1NvoatV0Hr"
      },
      "source": [
        "### **Predicting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzkh2qA4Svfc"
      },
      "source": [
        "Raw_new_data['Prediction'] = reg.predict(scaled_inputs)\n",
        "yhat_prob = reg.predict_proba(scaled_inputs)\n",
        "Raw_new_data['Probability_No'] = yhat_prob[:,0]\n",
        "Raw_new_data['Probability_Yes'] = yhat_prob[:,1]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK1aMEBef67F"
      },
      "source": [
        "### **Exporting The Predictions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr0nCeBhf8PV"
      },
      "source": [
        "Raw_new_data.to_csv('Predictions.csv')"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jGz_CFof8yp"
      },
      "source": [
        ""
      ],
      "execution_count": 99,
      "outputs": []
    }
  ]
}